{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import InputLayer, ReshapeLayer, GRULayer\n",
    "from lasagne.layers import DenseLayer\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDINGS_FN = os.path.expanduser(\"~/data/glove.6B.50d.txt\")\n",
    "def get_embeddings():\n",
    "    embedding_dict = dict()\n",
    "    embedding_mat = []\n",
    "    print(\"loading embeddings\")\n",
    "    with open(EMBEDDINGS_FN) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            word = parts[0]\n",
    "            v_t = np.array(parts[1:], dtype=np.float32)\n",
    "            embedding_mat.append(v_t)\n",
    "            embedding_dict[word] = len(embedding_mat)\n",
    "    unk = np.zeros_like(embedding_mat[0])\n",
    "    embedding_mat = [unk] + embedding_mat\n",
    "    embedding_mat = np.array(embedding_mat, dtype=np.float32)\n",
    "    print(\"done loading embeddings\")\n",
    "    return embedding_dict, embedding_mat, unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings\n",
      "done loading embeddings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.41800001,  0.24968   , -0.41242   , ..., -0.18411   ,\n",
       "        -0.11514   , -0.78580999],\n",
       "       [ 0.013441  ,  0.23682   , -0.16899   , ..., -0.56656998,\n",
       "         0.044691  ,  0.30392   ],\n",
       "       ..., \n",
       "       [-0.51181   ,  0.058706  ,  1.09130001, ..., -0.25003001,\n",
       "        -1.125     ,  1.58630002],\n",
       "       [-0.75897998, -0.47426   ,  0.47369999, ...,  0.78953999,\n",
       "        -0.014116  ,  0.64480001],\n",
       "       [ 0.072617  , -0.51393002,  0.47279999, ..., -0.18907   ,\n",
       "        -0.59021002,  0.55558997]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dict, embedding_mat, unk = get_embeddings()\n",
    "embedding_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_tweet(tweet, embedding_dict, embedding_mat):\n",
    "    tweet_embeds = [embedding_mat[embedding_dict.get(token, 0)] for token in tweet.split() if not token.isspace()]\n",
    "    return tweet_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.expanduser(\"~/data/twitter-datasets\")\n",
    "def load_data(embedding_dict, embedding_mat, full_data=False):\n",
    "    fns = ['train_pos', 'train_neg']\n",
    "    if full_data:\n",
    "        fns = [fn + '_full' for fn in fns]\n",
    "    \n",
    "    def load_from_file(fn):\n",
    "        with open(fn) as f:\n",
    "            return [embed_tweet(tweet, embedding_dict, embedding_mat) for tweet in f]\n",
    "    \n",
    "    data_pos, data_neg = [load_from_file(os.path.join(DATA_DIR, fn + '.txt')) for fn in fns]\n",
    "    X = data_pos\n",
    "    X.extend(data_neg)\n",
    "    Y = np.concatenate((np.ones(len(X) - len(data_neg), dtype=np.int64), np.zeros(len(data_neg), dtype=np.int64)))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198000\n"
     ]
    }
   ],
   "source": [
    "data_x_full, data_y_full = load_data(embedding_dict, embedding_mat)\n",
    "data_x, data_x_test, data_y, data_y_test = train_test_split(data_x_full, data_y_full, test_size=0.01)\n",
    "print(len(data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_network(input_var, dimensions=50):\n",
    "    net = InputLayer((1, None, dimensions), input_var=input_var)\n",
    "    net = GRULayer(net, 128, backwards=True)\n",
    "    net = GRULayer(net, 128, only_return_final=True)\n",
    "    net = DenseLayer(net, 2, nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(learning_rate=0.1, momentum=0.9, l2=0.00001):\n",
    "    input_var = T.tensor3('input', dtype='float32')\n",
    "    target_var = T.ivector('target')\n",
    "    network = build_network(input_var)\n",
    "\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    loss += lasagne.regularization.regularize_network_params(network, lasagne.regularization.l2) * l2\n",
    "\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "    acc = T.mean(T.eq(T.argmax(prediction, axis=1), target_var), dtype='float32')\n",
    "\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "    val_fn = theano.function([input_var, target_var], [loss, acc])\n",
    "    \n",
    "    return train_fn, val_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fn, val_fn = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for iteration in range(100000):\n",
    "    i = np.random.choice(len(data_x))\n",
    "    x, y = data_x[i], data_y[i]\n",
    "    loss = train_fn([x], [y])\n",
    "    if iteration % 10000 == 0:\n",
    "        print('performing validation')\n",
    "        loss, acc = 0., 0.\n",
    "        for dxt, dyt in zip(data_x_test, data_y_test):\n",
    "            loss_i, acc_i = val_fn([dxt], [dyt])\n",
    "            loss += loss_i\n",
    "            acc += acc_i\n",
    "        print('validation loss:', loss / len(data_x_test))\n",
    "        print('validation accuracy:', acc / len(data_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
